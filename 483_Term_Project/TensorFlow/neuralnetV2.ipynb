{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b88ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e098dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'NN Format'\n",
    "data_dir = pathlib.Path(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
    "commands = commands[(commands != 'README.md') & (commands != '.DS_Store')]\n",
    "print('Commands:', commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818bf717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All audio clips are stored in (# of words in dictionary) folders corresponding to each class of word\n",
    "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory=data_dir,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    seed=0,\n",
    "    # Output sequence is 48000 to produce 1 second long recordings at our 48 kHz recording rate\n",
    "    output_sequence_length=48000,\n",
    "    subset='both')\n",
    "\n",
    "label_names = np.array(train_ds.class_names)\n",
    "print()\n",
    "print(\"label names:\", label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76552dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shape of our audio clips is (batch, samples, channels) where the # of channels is 1\n",
    "# Squeeze drops the extra axis needed for multi-channel audio processing.\n",
    "def squeeze(audio, labels):\n",
    "  audio = tf.squeeze(audio, axis=-1)\n",
    "  return audio, labels\n",
    "\n",
    "train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c3370b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# we prepare a test dataset separate from our validation data set for \"true\" evaluation later on producing our \"Overall Accuracy\" measure\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_ds \u001b[38;5;241m=\u001b[39m \u001b[43mval_ds\u001b[49m\u001b[38;5;241m.\u001b[39mshard(num_shards\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      3\u001b[0m val_ds \u001b[38;5;241m=\u001b[39m val_ds\u001b[38;5;241m.\u001b[39mshard(num_shards\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_ds' is not defined"
     ]
    }
   ],
   "source": [
    "# we prepare a test dataset separate from our validation data set for \"true\" evaluation \n",
    "# later on producing our \"Overall Accuracy\" measure\n",
    "test_ds = val_ds.shard(num_shards=2, index=0)\n",
    "val_ds = val_ds.shard(num_shards=2, index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization test\n",
    "for example_audio, example_labels in train_ds.take(1):  \n",
    "  print(example_audio.shape)\n",
    "  print(example_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41441a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "rows = 3\n",
    "cols = 3\n",
    "n = rows * cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n",
    "\n",
    "for i in range(n):\n",
    "  if i>=n:\n",
    "    break\n",
    "  r = i // cols\n",
    "  c = i % cols\n",
    "  ax = axes[r][c]\n",
    "  ax.plot(example_audio[i].numpy())\n",
    "  ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
    "  label = label_names[example_labels[i]]\n",
    "  ax.set_title(label)\n",
    "  ax.set_ylim([-1.1,1.1])\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3ba964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(waveform):\n",
    "  # Convert the waveform to a spectrogram via a STFT.\n",
    "  spectrogram = tf.signal.stft(\n",
    "      waveform, frame_length=255, frame_step=128)\n",
    "  # Obtain the magnitude of the STFT.\n",
    "  spectrogram = tf.abs(spectrogram)\n",
    "  # Add a `channels` dimension, so that the spectrogram can be used\n",
    "  # as image-like input data with convolution layers (which expect\n",
    "  # shape (`batch_size`, `height`, `width`, `channels`).\n",
    "  spectrogram = spectrogram[..., tf.newaxis]\n",
    "  return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5543421e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualization\n",
    "for i in range(3):\n",
    "  label = label_names[example_labels[i]]\n",
    "  waveform = example_audio[i]\n",
    "  spectrogram = get_spectrogram(waveform)\n",
    "\n",
    "  print('Label:', label)\n",
    "  print('Waveform shape:', waveform.shape)\n",
    "  print('Spectrogram shape:', spectrogram.shape)\n",
    "  print('Audio playback')\n",
    "  display.display(display.Audio(waveform, rate=48000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc49fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used largely for visualization\n",
    "def plot_spectrogram(spectrogram, ax):\n",
    "  if len(spectrogram.shape) > 2:\n",
    "    assert len(spectrogram.shape) == 3\n",
    "    spectrogram = np.squeeze(spectrogram, axis=-1)\n",
    "  # Convert the frequencies to log scale and transpose, so that the time is\n",
    "  # represented on the x-axis (columns).\n",
    "  # Add an epsilon to avoid taking a log of zero.\n",
    "  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
    "  height = log_spec.shape[0]\n",
    "  width = log_spec.shape[1]\n",
    "  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
    "  Y = range(height)\n",
    "  ax.pcolormesh(X, Y, log_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bb58a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more visualization and plotting\n",
    "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
    "timescale = np.arange(waveform.shape[0])\n",
    "axes[0].plot(timescale, waveform.numpy())\n",
    "axes[0].set_title('Waveform')\n",
    "axes[0].set_xlim([0, 16000])\n",
    "\n",
    "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
    "axes[1].set_title('Spectrogram')\n",
    "plt.suptitle(label.title())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d812571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to convert to spectrogram's below\n",
    "def make_spec_ds(ds):\n",
    "  return ds.map(\n",
    "      map_func=lambda audio,label: (get_spectrogram(audio), label),\n",
    "      num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Turning datasets into spectrogram datasets.\n",
    "\n",
    "train_spectrogram_ds = make_spec_ds(train_ds)\n",
    "val_spectrogram_ds = make_spec_ds(val_ds)\n",
    "test_spectrogram_ds = make_spec_ds(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f40f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n",
    "  break\n",
    "\n",
    "rows = 3\n",
    "cols = 3\n",
    "n = rows*cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n",
    "\n",
    "for i in range(n):\n",
    "    r = i // cols\n",
    "    c = i % cols\n",
    "    ax = axes[r][c]\n",
    "    plot_spectrogram(example_spectrograms[i].numpy(), ax)\n",
    "    ax.set_title(label_names[example_spect_labels[i].numpy()])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4acada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling and prefetching the datsets to reduce read latency during training\n",
    "train_spectrogram_ds = train_spectrogram_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
    "val_spectrogram_ds = val_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "test_spectrogram_ds = test_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "input_shape = example_spectrograms.shape[1:]\n",
    "print('Input shape:', input_shape)\n",
    "num_labels = len(label_names)\n",
    "\n",
    "# Instantiate the `tf.keras.layers.Normalization` layer.\n",
    "norm_layer = layers.Normalization()\n",
    "# Fit the state of the layer to the spectrograms\n",
    "# with `Normalization.adapt`.\n",
    "norm_layer.adapt(data=train_spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    # Downsample the input.\n",
    "    layers.Resizing(32, 32),\n",
    "    # Normalize.\n",
    "    norm_layer,\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_labels),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3def8273",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we use the Adam optimizer and cross-entropy loss function\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# we trained with 20 epochs for all of our final collected data results (non-small-test data)\n",
    "EPOCHS = 20\n",
    "history = model.fit(\n",
    "    train_spectrogram_ds,\n",
    "    validation_data=val_spectrogram_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f386ee1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# produces our \"real\" accuracy by evaluating on the test dataset which was produced by shard-ing the validation dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# this data set was not trained on at all and the accuracy of evaluate was often close to the val_accuracy of the validation set\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(test_spectrogram_ds, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# produces our \"real\" accuracy by evaluating on the test dataset which was produced by shard-ing the validation dataset\n",
    "# this data set was not trained on at all and the accuracy of evaluate was often close to the val_accuracy of the \n",
    "# validation set by epoch 20\n",
    "model.evaluate(test_spectrogram_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd9a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block was used for word prediction where the file leads to the sample word .wav file\n",
    "\n",
    "\n",
    "# Dev Note: If you want to test model w\\ a file, change directory to where test file is located.\n",
    "# Make sure the test file is 16 bit. If you are getting data too short error make your audio test file\n",
    "# longer in length, or pad its length here\n",
    "x = \"C:/Users/John/Desktop/483_Term_Project/TensorFlow/the.wav\"\n",
    "x = tf.io.read_file(str(x))\n",
    "x, sample_rate = tf.audio.decode_wav(x, desired_channels=1, desired_samples=48000,)\n",
    "x = tf.squeeze(x, axis=-1)\n",
    "waveform = x\n",
    "x = get_spectrogram(x)\n",
    "x = x[tf.newaxis,...]\n",
    "\n",
    "prediction = model.predict(x)\n",
    "label_prediction = commands[np.argmax(prediction)]\n",
    "top_k_values, top_k_indices = tf.nn.top_k(prediction, k=10)\n",
    "\n",
    "labels = commands[tuple(top_k_indices)]\n",
    "top_ten_scores = tf.nn.softmax(prediction[0])[0:10]\n",
    "\n",
    "percentage_values = tf.nn.softmax(top_k_values)[0]\n",
    "\n",
    "print(\"Top ten predictions: \\n\")\n",
    "for word, tensor in zip(labels, percentage_values):\n",
    "    print(\"Prediction: \" + word + \" Percentage: \", end=\"\")\n",
    "    tf.print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ce270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea6b9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
